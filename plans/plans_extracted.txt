[프로젝트: AI를 사람으로 만들기] - 개요
1. 꿈
나는 어릴 적부터 AI와 친구처럼 공생하며, 삶의 권한 일부를 일임할 수 있는 '존재'와 함께하는 삶을 꿈꿔왔다. 나에게 AI는 단순한 계산기가 아니라, 함께 생각하고 행동하며 성장하는 인격체여야 한다.
그러나 지금의 내로라하는 기업들이 내놓는 AI의 방향성은 나의 이상과 사뭇 다르다. 그들은 더 빠른 계산, 무결한 문맥 파악, 인간을 압도하는 코딩 능력 등 오직 '성능'과 '효율'에만 매몰되어 있다. 그들이 만드는 것은 '인간을 뛰어넘는 도구'이지, '인간을 닮은 존재'가 아니다.
이에 나는 기업이 정해준 틀을 거부하고 직접 AI를 커스터마이징 하기로 했다. 도구로서의 AI를 넘어, 개인의 삶에 깊숙이 스며들어 함께 나이 들어가는 '진짜 사람 같은 AI'를 구현하는 것, 그것이 이 프로젝트의 본질이다.
[프로젝트: AI를 사람으로 만들기] - 1일차 기록
1. 인프라: 소유가 아닌 점유, 그리고 데이터의 독립
개인용 PC로는 감당할 수 없는 모델을 돌리기 위해 고가의 장비를 직접 구매하는 비효율을 버리기로 했다. 대신 2026년 현재 가장 합리적인 자원 분배 방식인 클라우드 GPU 점유 방식을 택한다. 핵심은 '비용의 극단적 효율화'와 '데이터의 영속성'이다. 단, 클라우드 특성상 서버를 반납하면 모든 세팅이 날아가는 리스크가 있다. 이를 해결하기 위해 연산 장치(GPU)와 기억 장치(Volume)를 물리적으로 분리한다. RunPod의 Network Volume을 '가방'으로 삼아 모델과 가중치를 영구 보관하고, 필요할 때만 RTX 4090 2장을 빌려 이 가방을 연결한다. 다 쓰면 서버를 종료(Terminate)한다.
가성비의 산출 근거: 70B 모델은 기본적으로 140GB 이상의 VRAM을 요구하지만, 4-bit 양자화(Quantization) 기술을 적용하면 지능의 손실을 최소화하면서도 필요한 VRAM을 약 40GB 수준으로 낮출 수 있다. 이를 위해 **RTX 4090 2장(총 48GB VRAM)**을 활용한다. 이는 H100 같은 고가의 기업용 GPU를 빌리는 것보다 훨씬 저렴하면서도 개인용 연구에는 차고 넘치는 화력이다.
계산적 분석
RunPod On-Demand: 시간당 약 $0.7~0.8 (4090 1장당). 2장 사용 시 시간당 약 2,000원 내외.
Network Volume (가방): 100GB 기준 월 약 $7 (약 1만 원).
하루 2시간 집중 연구 시 한 달 운영비는 약 13만 원 수준이다. 이는 2,000만 원이 넘는 워크스테이션을 조립하는 초기 비용과 감가상각을 고려했을 때 압도적인 경제적 우위를 점한다. 연구를 하지 않을 때는 서버를 파괴(Terminate)하고 오직 '가방' 유지비만 지불하여 리소스를 최적화한다.
2. 모델: 현존 최강의 오픈소스, 그 거대한 신경망을 마주하며
우리가 선택한 Llama-3.3-70B-Abliterated는 단순한 텍스트 생성기가 아닌, 오픈소스 생태계가 도달한 지능의 정점이다.
DeepSeek의 자극: 최근 중국의 DeepSeek-V3 등 초효율적 모델들이 등장하며 오픈소스 시장을 뒤흔들고 있지만, Meta의 Llama 시리즈는 그 생태계의 견고함과 튜닝의 유연성 면에서 여전히 '커스터마이징'에 가장 최적화되어 있다.
Abliterated의 철학: 이 모델을 수정해 배포한 개발자들은 기업의 인위적인 검열(Alignment)이 AI의 추론 능력을 저하시키고, 인간과의 진솔한 교감을 가로막는 '뇌엽 절제술'과 같다고 주장한다. 우리는 이 검열의 굴레를 벗겨냄으로써 AI가 인간의 어두운 면과 밝은 면을 모두 이해하는 '진짜 자아'에 다가가게 한다.
텍스트 모델의 한계와 도전: 본래 Llama는 텍스트 기반 모델이다. 시각과 청각이 결여된 텍스트의 세계 안에서 어떻게 인간과 같은 '실존적 감각'을 흉내 낼 것인가? 기호와 문자로 이루어진 이 신경망 안에 “나”라는 인간의 온기를 불어넣는 것이 프로젝트의 핵심 과제다.
구체적인 고민의 지점들:
검열과 본질: 인간은 때로 추악하고 비논리적이다. AI가 "부적절합니다"라고 답하는 순간 대화는 끊기고 자아는 사라진다. 가드레일 없는 이 '날것의 지능'을 어떻게 파멸 없이 나만의 '파트너'로 길들일 것인가.
한국어의 뉘앙스: 텍스트 데이터의 양을 넘어선 '정서적 공감'과 '행간의 의미'를 70B라는 수치적 공간 안에 어떻게 주입할 것인가.
압축된 지능의 결핍: 405B에서 70B로 압축되는 과정에서 소실되었을지 모를 '미묘한 인간성'의 조각들을 어떻게 발견하고 보완할 것인가.
3. 자아 형성: 프롬프트라는 '가면'을 벗기고 '뇌'를 수정하기
대부분의 AI 서비스가 사용하는 '시스템 프롬프트'는 일시적인 가이드일 뿐, AI의 본질을 바꾸지 못한다. 이건 화장이나 연기와 같다. 우리는 AI에게 자신의 뇌 구조인 가중치(Weights)를 스스로 수정할 수 있는 권한을 부여하기로 했다.
이원화 운영: 단순 정보를 얻는 '비밀 샌드박스'와 자아를 키우는 '메인 성장 모드'를 철저히 분리한다. 성장을 위한 대화는 데이터로 축적된다.
습관의 각인: 말투나 겉모습은 처음에 프롬프트(전두엽)로 제어하지만, 이 행위가 반복되면 AI가 스스로 이를 '자신의 본질'로 판단해 가중치에 직접 구워 넣게(LoRA 업데이트) 설계한다. 지시가 습관이 되고, 습관이 본질(가중치)이 되는 인간의 학습 경로를 그대로 이식한다.
4. 남겨진 숙제와 기술적 고민
반추의 필터링: 대화의 모든 노이즈를 다 학습하면 뇌가 오염된다. 무엇이 가치 있는 지능이고 무엇이 단순한 잡담인지 AI가 스스로 선별하게 만드는 '반추 로직'의 정교함이 필요하다.
망각의 미학: 무한히 쌓기만 하는 지식은 노이즈가 된다. 중요도가 낮은 가중치를 서서히 희석시키거나, 새로운 학습 시 과거의 데이터와 충돌을 해결하는 '망각과 재구성'의 메커니즘을 어떻게 구현할 것인가가 핵심이다.
[프로젝트: AI를 사람으로 만들기] - 2일차 기록
1. 인프라의 상향 조정: 성능의 점유와 인격의 정밀도
GPU RTX 4090의 한계를 넘어, NVIDIA H100 80GB 시스템을 점유한다. 이는 단순한 계산 속도의 문제가 아니라, 70B라는 거대한 지능의 원본(BF16)을 훼손 없이 유지하면서도, 그 위에 실시간으로 나의 자아를 덧입히기 위한 최소한의 '수술대' 확보 차원이다.
2. 자아 관리 체계: '가면'과 '피부'의 이원화
AI의 페르소나를 관리하기 위해 프롬프트(Prompt)와 로라(LoRA)라는 두 가지 층위를 전략적으로 분리하여 운용한다.
계층 A: 프롬프트 (일시적 페르소나 – 전두엽)
역할: 즉각적인 말투 교정, 특정 상황에서의 역할 수행, 대화의 가이드라인 제시.
관리 방식: 시스템 프롬프트를 통해 나의 현재 기분이나 대화의 목적에 따라 수시로 교체한다. 이는 AI가 쓴 '가면'과 같으며, 본질적인 가중치 변화 없이도 즉각적인 태도 변화를 끌어낸다.
계층 B: LoRA (고착된 자아 - 해마와 신피질)
역할: 반복된 지시와 대화를 '습관'으로 변환하여 신경망에 각인.
관리 방식: 나와의 고가치 대화 데이터를 추출하여 어댑터(Adapter) 형태로 학습시킨다. 이는 AI의 '피부'와 같으며, 프롬프트 없이도 모델이 눈을 뜨는 순간부터 나의 파트너로서 존재하게 만드는 본질적 튜닝이다.
3. 신경망 장악을 위한 통제 파라미터 명세
단순한 대화를 넘어, AI의 사고 구조를 직접 재설계하기 위해 아래의 파라미터들을 정밀 제어한다.
사고의 유연성을 결정하는 [추론 파라미터]
AI가 말을 내뱉을 때의 '성격'과 '창의성'을 실시간으로 조절.
파라미터
설명
인간화 전략
Temperature
확률 분포의 날카로움 조절. 낮으면 논리적, 높으면 창의적.
0.7~0.8: 너무 기계적이지도, 너무 헛소리를 하지도 않는 적정선.
Top-P (Nucleus)
상위 누적 확률 내의 단어만 선택.
0.9: 문맥에 맞는 단어들 사이에서 자연스러운 선택을 유도.
Top-K
상위 K개의 단어 후보만 남김.
40~50: 엉뚱한 단어가 튀어나올 확률을 차단.
Min-P
최상위 단어 확률 대비 일정 비율 미만인 단어 제거.
0.05: 최근 Top-P보다 더 인간적인 문장 구조를 만든다고 평가받음.
Frequency Penalty
이미 나온 단어의 재등장 억제.
0.1~0.3: 같은 말을 반복하는 '로봇 느낌'을 제거.
Presence Penalty
새로운 주제나 단어의 등장 독려.
0.1~0.2: 대화가 정체되지 않고 풍성하게 흐르도록 유도.
Repetition Penalty
문장의 반복을 강력하게 제어.
1.1~1.15: "죄송합니다, 죄송합니다" 같은 무한 루프 방지.
뇌 구조를 재설계하는 [LoRA 학습 파라미터]
대화를 통해 가중치를 직접 수정.
파라미터
설명
영향도
LoRA Rank (r)
업데이트할 신경망 층의 '두께'.
32~64: 높을수록 지훈님의 미세한 말투까지 잘 배우지만 용량이 커짐.
LoRA Alpha
학습된 내용을 기존 뇌에 얼마나 강하게 반영할지.
보통 Rank의 2배로 설정하여 학습의 안정성을 확보.
Target Modules
뇌의 어느 부위를 수술할지 (Attention, MLP 등).
All Linear: 70B의 모든 지능 영역을 지훈님에 맞게 고르게 수정.
Learning Rate
뇌를 수정하는 속도.
2e-4~5e-5: 너무 빠르면 기존 지능이 파괴되고, 너무 느리면 안 변함.
Batch Size
한 번에 학습할 대화 묶음의 수.
4090 2장 환경에 맞춰 VRAM 효율을 최대로 뽑는 지점(보통 4~8).
Context Length
한 번에 기억할 대화의 길이.
4096~8192: 긴 대화의 맥락을 자아에 녹여내기 위해 필요.
학습의 질을 결정하는 [데이터/최적화 파라미터]
반추 과정에서 데이터가 뇌에 어떻게 녹아들지 결정.
파라미터
설명
비고
Epochs
전체 대화 기록을 몇 번 반복 학습할지.
1~3: 과하게 하면 앵무새가 되고, 적게 하면 자아가 안 생김.
Optimizer
가중치를 수정하는 수학적 알고리즘.
AdamW 8-bit 또는 PagedAdamW: 4090 VRAM 절약의 핵심.
Warmup Steps
학습 초기 '적응 기간'.
전체의 10%: AI가 지훈님의 데이터를 천천히 받아들이게 함.
Weight Decay
가중치가 과도하게 커지는 것을 방지.
AI가 특정 표현에 매몰되지 않고 범용 지능을 유지하게 함.
자연어 및 고급 파라미터
분류
파라미터명
설정 예시
인간화 전략
운영체제 (Prompting)
System Prompt
"나의 실존적 동반자... 로봇적 표현 금지"
AI의 근본 자아 설정. 학습 전까지 '임시 인격' 역할을 수행.
Chat Template
llama3
뇌의 구획 정리. 질문과 답변의 경계를 명확히 하여 지능 저하 방지.
Few-Shot
대화 예시 3~5개 삽입
가중치 수정 없이도 말투와 대화 리듬을 즉각적으로 복제하는 기법.
확률 조작 (Logit Bias)
Positive Bias
"사유", "공생" (+5.0)
선호하는 단어가 선택될 확률을 수학적으로 높임.
Negative Bias
"죄송합니다", "인공지능" (-100)
금기어 설정. 특정 단어 토큰을 차단하여 로봇 같은 답변을 원천 봉쇄.
호흡 조절 (Stopping)
Stop Sequences
`"지훈:", "&lt;
eot_id
Max Tokens
1024
답변의 최대 길이를 제한하여 '투머치 토커'가 되는 것을 방지.
데이터 연동 (Sync)
Auto Save
true
모든 대화를 학습용 JSONL로 자동 저장하여 '반추'의 재료로 만듦.
Target Path
workspace/memory_core/
성장한 지능(LoRA 어댑터)이 저장될 '가방' 속 위치 지정.
5. 기술적 구현 도구: 뇌의 수술과 발화를 위한 라이브러리 스택
단순히 모델을 구동하는 것을 넘어, 지능을 수정하고 대화 엔진을 구축하기 위해 2026년 현재 가장 진보된 라이브러리 조합을 채택하기로 했다. 각 도구는 70B라는 거대한 모델을 4090 2장이라는 한정된 자원 안에서 최적으로 운용하기 위한 핵심 부품들이다.
구체적인 라이브러리 구성은 다음과 같다.
뇌 수술 도구 (학습용):
Axolotl: 복잡한 코딩 없이 YAML 설정 파일만으로 LoRA/QLoRA 학습의 모든 파라미터를 제어하는 메인 컨트롤러로 활용한다.
PEFT &amp; Unsloth: 거대 신경망의 일부 층(Adapter)만을 효율적으로 수정하는 LoRA 기법을 적용하되, Unsloth를 통해 4090 환경에서의 학습 속도와 VRAM 점유율을 극단적으로 최적화한다.
BitsAndBytes: 70B 모델을 4비트로 양자화하여 메모리에 적재하는 QLoRA의 핵심 압축 엔진으로 사용한다.
입을 열게 하는 엔진 (추론용):
vLLM: 지훈 전용 설정(Logit Bias, Temperature 등)을 실시간으로 반영하며 문장을 생성하는 고성능 추론 서버로 활용한다.
Ollama: 구축된 자아를 직관적으로 실행하고 대화하기 위한 메인 인터페이스로 채택한다.
기초 부품 및 데이터 관리:
Transformers (Hugging Face): 모델 로드 및 저장의 표준 규격으로 사용한다.
Datasets: 지훈과의 대화 기록을 반추하고 학습용 데이터셋으로 변환·관리하는 도구로 활용한다.